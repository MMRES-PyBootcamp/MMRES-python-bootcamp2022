{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MMRES-PyBootcamp/MMRES-python-bootcamp2021/blob/master/04_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Pandas (Secon part) TODO\n",
    "> An introduction on Pandas basics. TODO Here you will hear (just a bit) about Python *packages* and *modules*. Then you will be introduced to *lists* and *dictionaries*, some of the most versatile data types in Python. Finally, you will become familiar with the concept of *flow control* and the definition of your own *functions*. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline TODO\n",
    " * [DataFrame transformations](#DataFrame-transformations)\n",
    "   * [DataFrame numerical transformations](#DataFrame-numerical-transformations)\n",
    "   * [DataFrame text transformations](#DataFrame-text-transformations)\n",
    " * [Exporting DataFrames](#Exporting-DataFrames)\n",
    " * [Grouping-by and aggregating DataFrames](#Grouping-by-and-aggregating-DataFrames)\n",
    " * [Pivoting DataFrames](#Pivoting-DataFrames)\n",
    " * [Melting DataFrames](#Melting-DataFrames)\n",
    " * [User defined functions](#User-defined-functions)\n",
    " * [User defined functions](#User-defined-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice:</b> Practice cells announce exercises that you should try during the current boot camp session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Extension:</b> Extension cells correspond to exercises (or links to contents) that are a bit more advanced. We recommend to try them after the current boot camp session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Tip cells just give some advice or complementary information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>Caveat:</b> Caveat cells warn you about the most common pitfalls one founds when starts his/her path learning Python.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This document is devised as a tool to enable your self-learning process. If you get stuck at some step or need any kind of help, please don't hesitate to raise your hand and ask for the teacher's guidance.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame transformations\n",
    "\n",
    "We are now familiar on how to *access* the data stored in a DataFrame. Our next step will be how to *transform* such data. Let's begin again by loading Pandas with the `pd` alias and by importing `ToySpreadsheet.xlsx` from the `/MMRES-python-bootcamp2022/datasets` sub-folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load package with its corresponding alias\n",
    "import pandas as pd\n",
    "\n",
    "# Reading an Excel SpreadSheet and storing it in as a DataFrame called `df`\n",
    "df = pd.read_excel(io='datasets/ToySpreadsheet.xlsx')\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame numerical transformations\n",
    "\n",
    "Let's start by *standardizing* the values of a numerical column. By *standardizing* we mean taking a given distribution of values and bring it to a newer distribution with mean equal zero and standard deviation equal one. This *standardized* distribution is usually known as the [standard score](https://en.wikipedia.org/wiki/Standard_score) or *Z-score*. The $i$<sup>th</sup> observation of an $x$ magnitude, $(x_i)$, has a Z-score, $(Z_i)$, given by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Z_i = \\frac{x_i - \\mu(x)}{\\sigma(x)} ,\n",
    "\\end{equation}\n",
    "\n",
    "where, $\\mu(x)$ and $\\sigma(x)$ are the mean and the standard deviation of $x$, respectively. For example, let's get the Z-score of `df['Intensity']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of the 'Intensity': I_mean\n",
    "I_mean = df['Intensity'].mean()\n",
    "print(I_mean)\n",
    "\n",
    "# Get the standard deviation the 'Intensity': I_std\n",
    "I_std = df['Intensity'].std()\n",
    "print(I_std)\n",
    "\n",
    "# Computing the Z-score of the 'Intensity' column\n",
    "I_z = (df['Intensity'] - I_mean) / I_std\n",
    "\n",
    "# Storing it in a new 'Z-Intensity' column\n",
    "df['Z-Intensity'] = I_z\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how easy is:\n",
    "* To operate with a Pandas Series and numeric constants stored in variables: `(df['Intensity'] - I_mean) / I_std`.\n",
    "* To store a freshly created Series `I_z` into a pre-existing DataFrame `df` with a new column name `'Z-Intensity'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice:</b>\n",
    "\n",
    "The $i$<sup>th</sup> observation of an $x$ magnitude, $(x_i)$, has a 0-to-1 normalization, $(N_i)$, given by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "N_i = \\frac{x_i - m(x)}{M(x) - m(x)},\n",
    "\\end{equation}\n",
    "\n",
    "where, $m(x)$ and $M(x)$ are the minimum and the maximum values of $x$, respectively.\n",
    "    \n",
    "1) In the 1<sup>st</sup> code cell below, compute the 0-to-1 normalization of `df['Amplitude']`. \n",
    "    \n",
    "Un-comment and fill only those code lines with underscores `___` or `print()`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ejercicio",
     "error"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the minimum of the 'Amplitude': A_min\n",
    "#A_min = ___\n",
    "#print(A_min)\n",
    "\n",
    "# Get the mnaximum of the 'Amplitude': A_max\n",
    "#A_max = ___\n",
    "#print(A_max)\n",
    "\n",
    "# Computing the N-normalization of the 'Amplitude' column and storing it in a new 'N-Amplitude' column\n",
    "#df['N-Amplitude'] = ___\n",
    "\n",
    "# Return the DataFrame\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "ejercicio",
     "solucion"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the minimum of the 'Amplitude': A_min\n",
    "A_min = df['Amplitude'].min()\n",
    "print(A_min)\n",
    "\n",
    "# Get the mnaximum of the 'Amplitude': A_max\n",
    "A_max = df['Amplitude'].max()\n",
    "print(A_max)\n",
    "\n",
    "# Computing the N-normalization of the 'Amplitude' column and storing it in a new 'N-Amplitude' column\n",
    "df['N-Amplitude'] = (df['Amplitude'] - A_min) / (A_max - A_min)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's now devote some time in arranging `df` a bit more. For example, now that we have `'Z-Intensity'` and `'N-Amplitude'` we could discard `'Intensity'` and `'Amplitude'` using the [`.drop()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant columns 'Intensity' and 'Amplitude'\n",
    "list_drop = ['Intensity', 'Amplitude']\n",
    "df = df.drop(columns=list_drop)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, is a good practice to use nice *self explanatory* labels for DataFrame columns. However, it is also recommended to use labels as *short* as possible (try to find your balance between self explanatory and short). With this in mind, let's update some column labels from `df` using the [`.rename()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a renaming dictionary for incomming column rename\n",
    "dic_rename = {'Software': 'Soft', 'Sequence': 'Seq', 'Z-Intensity': 'I', 'N-Amplitude': 'A'}\n",
    "# Key (Old name). Value (New name)\n",
    "\n",
    "# Renaming some columns from `df`\n",
    "df = df.rename(columns=dic_rename)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember that dictionaries were know as a *mapping data types*? When calling `df.rename(columns=dic_rename)`, we used `dic_rename` to *map* old (*keys*) to new (*values*) column labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame text transformations\n",
    "\n",
    "Sometimes is useful to use text transformations on a given DataFrame column. For example, look at the column `'Raw'`. The strings within this column have a well organized structure comprising multiple substrings joined with underscores (`_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return 'Raw' column as a Series\n",
    "df['Raw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we have a date (`1985-04-06`), a four-digit code (`0123`), a two-letters code (`GA`), some kind of single letter indicator (`T` / `C`), and another correlative indicator (`R1` / `R2` / `R3` / `R4`). Let's integrate this info as `df` independent columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the 'Raw' column by the underscore '_'\n",
    "df['Split raw'] = df['Raw'].str.split('_')\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we get a new column called `'Split raw'` that has lists within! To achieve this we first used the accessor method [`.str`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.html) to *access* the strings stored in column `'Raw'`. Then, we chained the string method [`.split()`](https://docs.python.org/3/library/stdtypes.html#str.split) which (as you already know) returns lists. Now we should access the substrings stored within the lists stored in column `'Split raw'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the 1st element of the list in 'Split raw' as 'Date'\n",
    "df['Date'] = df['Split raw'].str[0]\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a new column `'Date'` with the information we were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice:</b>\n",
    "    \n",
    "1) In the 1<sup>st</sup> code cell below, get a new column called `'ID'` for the four-digit code (`0123`); a new column called `'User'` for the two-letters code (`GA`); a new column called `'Cond'` for the single letter indicator (`T` / `C`); and a new column called `'Rep'` for the correlative indicator (`R1` / `R2` / `R3` / `R4`).\n",
    "    \n",
    "2) In the 2<sup>nd</sup> code cell below, discard columns `df['Raw']` and `df['Split raw']`.\n",
    "\n",
    "    \n",
    "    \n",
    "Un-comment and fill only those code lines with underscores `___`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ejercicio"
    ]
   },
   "outputs": [],
   "source": [
    "# Taking 2nd, 3rd, 4th and 5th elements of the list in 'Split raw' as 'ID', 'User', 'Cond' and 'Rep'\n",
    "#df['ID'] = ___\n",
    "#df['User'] = ___\n",
    "#df['Cond'] = ___\n",
    "#df['Rep'] = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "ejercicio",
     "solucion"
    ]
   },
   "outputs": [],
   "source": [
    "# Taking 2nd, 3rd, 4th and 5th elements of the list in 'Split raw' as 'ID', 'User', 'Cond' and 'Rep'\n",
    "df['ID'] = df['Split raw'].str[1]\n",
    "df['User'] = df['Split raw'].str[2]\n",
    "df['Cond'] = df['Split raw'].str[3]\n",
    "df['Rep'] = df['Split raw'].str[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping redundant columns 'Raw' and 'Split raw'\n",
    "#list_drop = ___\n",
    "#df = df.drop(columns=___)\n",
    "\n",
    "# Return the DataFrame\n",
    "#___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant columns 'Raw' and 'Split raw'\n",
    "list_drop = ['Raw', 'Split raw']\n",
    "df = df.drop(columns=list_drop)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `df` is tidy enough as to be exported and locally stored. Look how easy is to save a DataFrame into our hard-disk with the method [`.to_excel()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the DataFrame as an Excel SpreadSheet\n",
    "df.to_excel(excel_writer='datasets/DataFrameSpreadsheet.xlsx', sheet_name='Excel_df', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping-by and aggregating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame method [`.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) is one of the most useful to start diving in your data. A group-by-and-aggregate operation takes place is three steps.\n",
    "\n",
    "1) DataFrame rows are **grouped by** the categories within a given column (or columns).\n",
    "2) The column (or columns) we want to aggregate are accessed.\n",
    "3) The accessed columns are then **aggregated** using an aggregating function.\n",
    "\n",
    "For example, suppose that we would like to know the mean `'I'` and `'A'` according to each `Soft`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Soft' and aggregating with mean\n",
    "df_g = df.groupby(by=['Soft'])[['I', 'A']].mean()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, maybe we would like to know the mean `'Intensity'` and `'Amplitude'` according to each `Node`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Soft' and aggregating with mean\n",
    "df_g = df.groupby(by=['Node'])[['I', 'A']].mean()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wen can also group-by multiple columns to have the information a bit more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Soft', 'Node' and aggregating with mean\n",
    "df_g = df.groupby(by=['Soft', 'Node', 'RNA'])[['I', 'A']].mean()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that grouping by `'RNA'` is not really necessary with this DataFrame (we have a single RNA sequence instead of multiple RNA sequences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice:</b>\n",
    "    \n",
    "1) In the 1<sup>st</sup> code cell below, group `df` by `'Soft'`, `'Node'`, `'RNA'`, and aggregate `'A'` with the minimum. Store the \"grouped-by-and-aggregated\" DataFrame as `df_g_Amin`.\n",
    "    \n",
    "2) In the 2<sup>nd</sup> code cell below, group `df` by `'Soft'`, `'Node'`, `'RNA'`, and aggregate `'I'` with the maximum. Store the \"grouped-by-and-aggregated\" DataFrame as `df_g_Imax`.\n",
    "    \n",
    "Un-comment and fill only those code lines with underscores `___`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Soft', 'Node', 'RNA' and aggregating 'A' with min\n",
    "#df_g_Amin = ___\n",
    "\n",
    "# Return the DataFrame\n",
    "#___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grouping by 'Soft', 'Node', 'RNA' and aggregating 'A' with min\n",
    "df_g_Amin = df.groupby(by=['Soft', 'Node', 'RNA'])[['A']].min()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g_Amin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Soft', 'Node', 'RNA' and aggregating 'I' with max\n",
    "#df_g_Imax = ___\n",
    "\n",
    "# Return the DataFrame\n",
    "#___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grouping by 'Soft', 'Node', 'RNA' and aggregating 'I' with max\n",
    "df_g_Imax = df.groupby(by=['Soft', 'Node', 'RNA'])[['I']].max()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g_Imax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `.groupby()` method on a DataFrame gives a DataFrameGroupBy object that has another method called [`.agg()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html). This method is useful when we want to use multiple aggregating functions at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of columns to group by with\n",
    "list_gby = ['Soft', 'Node', 'RNA']\n",
    "\n",
    "# Creating a list of columns to aggregate\n",
    "list_agg = ['A', 'I']\n",
    "\n",
    "# Creating a list with string function names to aggregate with\n",
    "list_funs = ['min', 'max']\n",
    "\n",
    "# Group by and aggregate with multiple fuctions\n",
    "df_g = df.groupby(by=list_gby)[list_agg].agg(func=list_funs)\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now we get the minimum and the maximum for both columns `I` and `A`. Since we just want maximum `I` and minimum `A`, it would be great to specify which aggregating functions we want for each column. We can achieve this with a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary specifying how to aggregate each column\n",
    "dict_aggfuns = {'A': 'min', 'I': 'max'}\n",
    "\n",
    "# Group by and aggregate specifying how to aggregate each column\n",
    "df_g = df.groupby(by=list_gby).agg(func=dict_aggfuns)\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Extension:</b>\n",
    "\n",
    "We can specify the aggregating functions passed to the `.agg()` method as:\n",
    "1) Lists of builtin functions (like `min`, `max`).\n",
    "2) Lists of functions from packages (like `np.min`, `np.max`).\n",
    "3) Lists of \"function string names\" (like `'min'`, `'max'`).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are an experienced spreadsheet user, maybe you will find more familiar the term \"pivot table\" rather than \"grouping-by and aggregating\". In general, all that can be achieve by grouping-by-and-aggregating can be also be done with the [`.pivot_table()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by and aggregate with multiple fuctions\n",
    "df.groupby(by=list_gby).agg(func=dict_aggfuns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting and aggregating with multiple fuctions\n",
    "df.pivot_table(index=list_gby, aggfunc=dict_aggfuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the perfect correspondence between the `by=` parameter from `.groupby()` and the `index=` parameter from `.pivot_table()`, and similarly, between the `func=` parameter from the `.groupby()` method `.agg()` and the `aggfunc=` parameter from `.pivot_table()`. One distinguishing feature of `.pivot_table()` is the parameter `column=`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of columns to group by with\n",
    "list_indexes = ['RNA']\n",
    "\n",
    "# Creating a list of columns to group by with\n",
    "list_columns = ['Soft', 'Node']\n",
    "\n",
    "df.pivot_table(index=list_indexes, columns=list_columns, aggfunc=dict_aggfuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying `columns=`, we can now split `'Soft'` and `'Node'` categories by means of DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index=list_indexes, columns=list_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melting DataFrames\n",
    "\n",
    "In a \"[Tidy DataFrame](https://www.jstatsoft.org/article/view/v059i10)\", each variable is a column and each observation is a row. The Pandas function [`melt()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html) allows to switch from a \"Non-tidy DataFrame\" to a \"Tidy DataFrame\" very easily. Since our example DataFrame `df` is quite tidy, let's rename columns `'I'` and `'A'` just to better illustrate how does `melt()` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a renaming dictionary for incomming column rename\n",
    "dic_rename = {'I': 'Cat', 'A': 'Dog'}\n",
    "# Key (Old name). Value (New name)\n",
    "\n",
    "# Renaming some columns from `df`\n",
    "df = df.rename(columns=dic_rename)\n",
    "\n",
    "# Return the DataFrame (BEFORE melting)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks like that we have rows that mix observations for `'Cat'` and `'Dog'`. Let's melt this \"Non-tidy DataFrame\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melting 'Cat' and 'Dog', keeping 'Soft', 'Node', 'RNA', 'Cond' and 'Rep'\n",
    "df_melt = pd.melt(frame=df,\n",
    "                  id_vars=['Soft', 'Node', 'RNA', 'Cond', 'Rep'],\n",
    "                  value_vars=['Cat', 'Dog'],\n",
    "                  var_name='Animal',\n",
    "                  value_name='Score')\n",
    "\n",
    "# Return the DataFrame (AFTER melting)\n",
    "df_melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in `df_melt`, each row is an observation and each column is a variable. Note the arguments we used in `pd.melt()`:\n",
    " + `id_vars=`: List of columns to use as identifiers on the \"melted\" DataFrame.\n",
    " + `value_vars=`: List of columns to \"melt\".\n",
    " + `var_name=`: String to name \"melted\" columns.\n",
    " + `value_name=`: String to name \"melted\" values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite pivot tables are easier to inspect at a glance than Tidy DataFrames, it is always recommended to work with *tidy data*. In the boot camp session that we will devote to data visualization on September 21 <sup>st</sup> (16:00-17:00), we will see that many Python plotting functions work better with \"Tidy DataFrames\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Apply()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qECGYMhFwe_0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Pandas:', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dRXlNyzwe_1"
   },
   "outputs": [],
   "source": [
    "animals = ['Tiger', 'Bear', 'Moose']\n",
    "pd.Series(animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dW0u1aYnwe_1"
   },
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3]\n",
    "pd.Series(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKZxbs6swe_2"
   },
   "source": [
    "Notice that the series is indexed by default by integers. We can change this indexing by using a dictionary instead of a list to create the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfQllZnnwe_2"
   },
   "outputs": [],
   "source": [
    "sports = {'Archery': 'Bhutan',\n",
    "          'Golf': 'Scotland',\n",
    "          'Sumo': 'Japan',\n",
    "          'Taekwondo': 'South Korea'}\n",
    "s = pd.Series(sports)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMoMhXjdwe_3"
   },
   "source": [
    "On the other hand, dataframes can be built from two-dimensional arrays, with the ability of labelling columns and indexing the rows. **Every column in a dataframe is a series**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH2espiFwe_3"
   },
   "outputs": [],
   "source": [
    "# Sampling a 1000 rows 6 cols 2D array from the standard normal distribution and creating DataFrame\n",
    "u = pd.DataFrame(np.random.randn(1000, 6),\n",
    "                 index=np.arange(0, 3000, 3),\n",
    "                 columns=['A', 'B', 'C', 'D', 'E', 'F'])\n",
    "\n",
    "print(type(u))\n",
    "\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOr-n8Lbwe_4"
   },
   "source": [
    "As you might have noticed, it is not the best to look at massive dataframes. There are some functions that allow us to have a nicer look at parts of the dataframe to have an idea of \"how things are going\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QszUh5dVwe_4"
   },
   "outputs": [],
   "source": [
    "u.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSgZlKyzwe_5"
   },
   "outputs": [],
   "source": [
    "u.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLfIvV_Awe_5"
   },
   "outputs": [],
   "source": [
    "u.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-peS5fHwe_5"
   },
   "outputs": [],
   "source": [
    "u.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkB-VPofwe_6"
   },
   "source": [
    "### Indexing/Slicing in Pandas\n",
    "\n",
    "The easiest way to access information in a Pandas dataframe, equivalent to the way used in NumPy, is using the `iloc` command. With `iloc` we can use the same indexing techniques that we saw with NumPy in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f__CcBkTwe_6"
   },
   "outputs": [],
   "source": [
    "# Slice-in rows index 125 to 132 (132 included!) from columns index 0, 2 and 5\n",
    "u.iloc[125:132, [0, 2, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYOudgEGwe_6"
   },
   "source": [
    "We can choose specific columns according to their names using `loc` instead of `iloc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZE_agQpXwe_6"
   },
   "outputs": [],
   "source": [
    "# Slice-in rows 375 to 393 (393 included!) from columns A, C and F\n",
    "u.loc[375:393, ['A', 'C', 'F']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkotgoA8we_6"
   },
   "source": [
    "However, there are a few different ways of accessing the data in a Pandas dataframe, that typically have a more \"direct\" connection with the actual content fo the dataframe. Individual or sets of columns can also be accessed by their column names. Choosing one single column will give a Series, while two or more will produce a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ4tGTPFwe_7"
   },
   "outputs": [],
   "source": [
    "u['A'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HID7Eb0dwe_7"
   },
   "outputs": [],
   "source": [
    "u[['A', 'D']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCECWud4we_7"
   },
   "source": [
    "Not only that, we can access a single column without the need of brackets []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlcbT8xPwe_7"
   },
   "outputs": [],
   "source": [
    "u.A.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hotY3ZvIwe_7"
   },
   "source": [
    "Or, we can retrieve the elements that satisfy some condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTUZE2nzwe_8"
   },
   "outputs": [],
   "source": [
    "u[u.D > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znDPNtk-we_8"
   },
   "source": [
    "Dataframes provide the `query` functionality for the same purpose. While it is less powerful than boolean indexing, it is often faster and shorter (when names are longer than just `u`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfHCORHbwe_8"
   },
   "outputs": [],
   "source": [
    "u.query('D > 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVuI3r-Twe_8"
   },
   "source": [
    "### Reshaping `DataFrame`\n",
    "\n",
    "We can reshape and concatenate dataframes in a pretty similar way to numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9Srkvq5we_8"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "\n",
    "df1['sample'] = ['A', 'A', 'A', 'B', 'B', 'B']\n",
    "df1['replicate'] = ['01', '02', '03', '01', '02', '03']\n",
    "df1['protein'] = 'P02768'\n",
    "df1['value1'] = np.random.randn(6)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Rh2bHncwe_8"
   },
   "outputs": [],
   "source": [
    "pivot_df1 = df1.pivot(index='replicate', columns='sample', values='value1')\n",
    "\n",
    "pivot_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UDugR2Twe_9"
   },
   "source": [
    "### Computing With `DataFrames`\n",
    "\n",
    "We can calculate with `DataFrames` or their columns (which are `Series`) the same way we would work with numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kr4LjYXqwe_9"
   },
   "outputs": [],
   "source": [
    "df1['value2'] = 1 / df1['value1']\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kz_Oravuwe_9"
   },
   "outputs": [],
   "source": [
    "np.mean(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-4MfgiPwe_9"
   },
   "source": [
    "We can also apply functions to the whole dataset or specific columns with the `apply` command. `apply` acts on the whole column at a time (i.e. a Pandas `Series`), so we can compute things that depend on several values of the column, for instance, the mean value. To apply functions in a real element-by-element basis the function `applymap` or `Series.apply` should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AX8bRzhDwe_9"
   },
   "outputs": [],
   "source": [
    "def mean(col):\n",
    "    return sum(col) / len(col)\n",
    "\n",
    "df1[['value1', 'value2']].apply(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UA0KJsYqwe_9"
   },
   "source": [
    "While most can be directly calculated (including the given example of the mean), `apply` also works on columns with strings or categorical data, where no mathematical operations are defined. The limit is the imagination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5ONVli7we_9"
   },
   "source": [
    "### Combining `DataFrames`\n",
    "\n",
    "Something we will do quite often as scientists is combining data from different sources into one single source. This can be achieved by different commands in Pandas, depending on the actual goal we want.\n",
    "\n",
    "To begin with, appending new rows of data is achieved by the command `append`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWuaqmG6we_-"
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "\n",
    "df2['sample'] = ['A', 'A', 'A', 'B', 'B', 'B']\n",
    "df2['replicate'] = ['01', '02', '03', '01', '02', '03']\n",
    "df2['protein'] = 'P69892'\n",
    "df2['value1'] = np.random.randn(6)\n",
    "df2['value2'] = 1 / df2['value1']\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrVuQ26xwe_-"
   },
   "outputs": [],
   "source": [
    "df1.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te0sJ4xYwe_-"
   },
   "source": [
    "The same result can be obtained with `concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whMeHgI1we_-"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5WZ6nRVwe_-"
   },
   "source": [
    "### Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ji6npHcMwe_-"
   },
   "outputs": [],
   "source": [
    "df.groupby('protein').agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4gLCDWYwe_-"
   },
   "outputs": [],
   "source": [
    "df.groupby(['protein', 'sample']).agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-5yICjnwe_-"
   },
   "outputs": [],
   "source": [
    "df.groupby(['protein', 'sample', 'replicate']).agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s3-1l8Lwe__"
   },
   "outputs": [],
   "source": [
    "df.groupby('protein').transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKHk3OU7we__"
   },
   "outputs": [],
   "source": [
    "df.groupby('protein')['value1', 'value2'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRQpvJOIwe__"
   },
   "outputs": [],
   "source": [
    "for g, g_df in df.groupby(['protein', 'sample']):\n",
    "    print(g_df)\n",
    "    print(f\"{g} --> mean value1: {np.mean(g_df['value1'])}\")\n",
    "    print(f\"      mean value2: {np.mean(g_df['value2'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zh5TW5Ewe__"
   },
   "outputs": [],
   "source": [
    "df.groupby(['protein', 'sample']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZymA9jdswe__"
   },
   "outputs": [],
   "source": [
    "df.pivot_table(index='protein',\n",
    "               columns='sample', \n",
    "               aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-V5VY3mGwfAA"
   },
   "outputs": [],
   "source": [
    "df.pivot_table(index='protein',\n",
    "               columns='sample',\n",
    "               aggfunc={'value1': min,\n",
    "                        'value2': max})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBie1CnhwfAA"
   },
   "source": [
    "### Loading and saving dataframes\n",
    "\n",
    "To load and save Pandas dataframes we will use the `to_csv` and `read_csv` commands. Whenever the dataframe does not contain any kind of column that is of type `object` we can also use feather format with `to_feather`. In case we have objects in the cells, such as functions, for example, we can use pickle format with `to_pickle`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6tARwXFwfAA"
   },
   "outputs": [],
   "source": [
    "df.to_csv('test.csv')\n",
    "pd.read_csv('test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiGpBxl3wfAA"
   },
   "source": [
    "But, as an addition, Pandas has special commands to load and save Excel spreadsheets (yay!). However, to use it you'll need the `openpyxl` and `xlrd` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGdlb3UvwfAA"
   },
   "outputs": [],
   "source": [
    "df.to_excel('test.xlsx', sheet_name='My sheet')\n",
    "pd.read_excel('test.xlsx', 'My sheet', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q3OWleewfAA"
   },
   "source": [
    "**Exercise 5**: Download [this dataset](https://raw.githubusercontent.com/ChihChengLiang/pokemongor/master/data-raw/pokemons.csv) and load it, using the first column as the index. Take a look at it, and do the following things:\n",
    "- Choose the columns 'Identifier', 'BaseStamina', 'BaseAttack', 'BaseDefense', 'Type1' and 'Type2' \n",
    "- Create a function that lowercases strings and apply it to 'Type1' and 'Type2' (*Extra: just capitalize the strings, i.e., leave the first letter uppercase and lowercase the rest*)\n",
    "- Create a function that returns a Boolean value (don't be afraif by this, it is a function that returns either True or False) that tells if a Pokémon has high stamina (BaseStamina>170) or not. Store this information in a new column and show the list of Pokémon with high stamina\n",
    "- Show the instructor the last 15 rows of your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUPGHqP2wfAA"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ChihChengLiang/pokemongor/master/data-raw/pokemons.csv', \n",
    "                 index_col=0)\n",
    "\n",
    "df = df[['Identifier', 'BaseStamina', 'BaseAttack', 'BaseDefense', 'Type1', 'Type2']]\n",
    "\n",
    "capitalize = lambda st: st.capitalize()\n",
    "\n",
    "for col in ['Type1', 'Type2']:\n",
    "    df[col] = df[col].apply(capitalize)\n",
    "    \n",
    "def highstamina(x):\n",
    "    return True if x > 170 else False\n",
    "\n",
    "df['HighStamina'] = df.BaseStamina.apply(highstamina)\n",
    "\n",
    "print(df[df['HighStamina'] == True].Identifier)\n",
    "\n",
    "df.tail(15)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "04_Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
