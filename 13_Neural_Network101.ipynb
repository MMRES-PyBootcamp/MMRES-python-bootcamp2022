{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d487747-246d-43d3-954a-0c08fdae2d05",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MMRES-PyBootcamp/MMRES-python-bootcamp2022/blob/main/13_Neural_Network101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e297d74b-28cb-4403-82c3-70917fbf7855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qottmann/anaconda3/envs/test/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd1c9a-2dc5-4d67-93b8-54f198760414",
   "metadata": {},
   "source": [
    "To build a neural network in (py)torch, you are expected to fill a \"template\" class  \n",
    "that has a `__init__` and `forward` method. This varies for different machine learning  \n",
    "frameworks, but the overall logic is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c9db8d-b0ee-4d3e-b5a6-124b85c41d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 28*28\n",
    "out_dim = 7\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.neural_network = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),  # input dimension, hidden1 dimension\n",
    "            nn.ReLU(),               # Non-linear activation function\n",
    "            nn.Linear(512, 512),     # hidden1 dimension, hidden2 dimension\n",
    "            nn.ReLU(),               # Non-linear activation function\n",
    "            nn.Linear(512, out_dim), # hidden2 dimension, output dimension\n",
    "            #nn.Softmax(dim=1)        # special output activation function\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)          # input data preparation\n",
    "        output = self.neural_network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe9e013-74ff-4b10-b2be-fd03becbc7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (neural_network): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab69d8-01f7-41af-910a-6fc955a0f65d",
   "metadata": {},
   "source": [
    "For parallelization, PyTorch (and other ML frameworks) are built to process data in batches.  \n",
    "Therefore, the input is always assumed to have the first dimension to be the `batch_size`.  \n",
    "If we want to process a single input, we need to accont for a dummy dimension `batch_size=1`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a0a90f-2061-47a7-8c73-0d2819e5bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X = torch.rand(28*28)\n",
    "    output = model(X) # results in an error\n",
    "except:\n",
    "    X = torch.rand(1, 28*28)\n",
    "    output = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd46f0ad-d3ed-46d2-9d2b-a0493069f1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b4ae2-44d3-4d74-bbea-951715d745c4",
   "metadata": {},
   "source": [
    "E1: Due to the `flatten` function in the forward pass, the input is rather flexible, try inputting alternatively shaped tensors.  \n",
    "Here is one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ea8826-5566-405f-be65-be7d483b9884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0126, -0.0991, -0.0260,  0.0468,  0.1808,  0.0109,  0.0938]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(1, 2, 14, 28)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd149842-c9d8-404a-a9cf-165197674ae8",
   "metadata": {},
   "source": [
    "E2: We want the output of our NN to be the probability distribution for `out_dim = 7` different categories.  \n",
    "Modify the network such that the output resembles a proper probability distribution, i.e. that \n",
    "$$\\sum_{i=1}^\\text{out_dim} p_i = 1$$ \n",
    "and $1\\geq p_i \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39c0ea4-215f-434a-81d9-08687acfb72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(torch.sum(model(X)), torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27bb0f62-bd08-4b92-92c4-aa59431b67ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(model(X)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3c46b-641e-4622-a41b-4595040e6613",
   "metadata": {},
   "source": [
    "(Note that you can use most of the standard `numpy` functions in torch)  \n",
    "Hint: Look up the so-called softmax function $\\text{Softmax}: \\mathbb{R}^m \\mapsto \\mathbb{R}^m$\n",
    "$$\n",
    "\\left(\\text{Softmax}(x)\\right)_i = \\frac{e^{x_i}}{\\sum_{j=1}^m e^{x_j}}\n",
    "$$\n",
    "for $x \\in \\mathbb{R}^m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cee2ca-8549-48b6-a004-6aa7f32a1307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
